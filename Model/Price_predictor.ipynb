{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OkzcpVUjvsee"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import RobustScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.model_selection import cross_val_score\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"]},{"cell_type":"code","source":["df=pd.read_csv(\"/content/drive/MyDrive/Dataset/HousingData.csv\")"],"metadata":{"id":"siVw5lRnynDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.rename(columns={'MEDV':'Price'}, inplace=True)\n","df.head()"],"metadata":{"id":"GtEHk4qB0GgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.tail()"],"metadata":{"id":"HZqjaHwNzPRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe().T"],"metadata":{"id":"vxzAeP7QzTme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"wplI-k3s4Ox9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"mXiHSulMzaLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.size"],"metadata":{"id":"7VeD1lxLzizP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"BoxDOV-deatR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isna().sum()"],"metadata":{"id":"LtvdWnGbz1Gh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Handling Null Values\n","\n","The dataset contains missing (null) values. To handle them, we have two main approaches:  \n","\n","1. **Drop Rows**: Remove rows containing null values.  \n","2. **Impute Missing Values**: Replace null values with appropriate statistical measures:  \n","   - **Numerical Columns**: Use **mean** or **median** for replacement.  \n","   - **Categorical Columns**: Use **mode** (most frequent value) for replacement.  \n","\n","Using these strategies ensures that the dataset remains clean and useful for analysis and modeling."],"metadata":{"id":"hujQyJzGe-ab"}},{"cell_type":"code","source":["for col in df.columns:\n","    if df[col].dtype != 'object':  # If column is numeric\n","        df[col] = df[col].fillna(df[col].median())  # Fill with median\n","\n","\n","df.isna().sum()\n"],"metadata":{"id":"_o3r8ZxXfOfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.duplicated().sum()"],"metadata":{"id":"S04y9GHvgAPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","np.isinf(df).sum()\n"],"metadata":{"id":"j3IvHqDzhHby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_cols=df.select_dtypes(include=np.number).columns\n","categorical_cols=df.select_dtypes(include='object').columns\n","\n","print(\"Number of mumeric cols: \",len(numeric_cols))\n","print(\"Number of categorical cols: \",len(categorical_cols))"],"metadata":{"id":"ZovDvNKlmP3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["col_name=[]\n","n_unique=[]\n","unique_value=[]\n","col_types=[]\n","for col in df.columns:\n","    col_name.append(col)\n","    n_unique.append(len(df[col].unique()))\n","    unique_value.append(df[col].unique())\n","    col_types.append(df[col].dtype)\n","\n","check_dic={\"col_name\":col_name,\"no of unique vlaues\":n_unique,\"unique_value\":unique_value,\"col_types\":col_types}\n","check_df=pd.DataFrame(check_dic)\n","check_df\n"],"metadata":{"id":"ygLWBZKRmtHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data visualization\n","## Exploring Categorical Variables\n","\n","In this dataset, we can consider both **CHAS** and **RAD** as categorical variables:  \n","\n","- **CHAS**: A binary variable indicating whether the property is next to the Charles River (**1 = Yes, 0 = No**).  \n","- **RAD**: An index representing accessibility to radial highways, which can be treated as a categorical feature due to its discrete nature.  \n","\n","Properly handling these categorical variables can improve model performance in predictive tasks."],"metadata":{"id":"MwcLfwcvoIEK"}},{"cell_type":"code","source":["cols=['CHAS','RAD']\n","for col in cols:\n","    plt.figure(figsize=(10,4))\n","    sns.countplot(x=col,data=df)\n","    plt.title(f'Bar Chart of {col}')\n","    plt.grid(True)\n","    plt.show()"],"metadata":{"id":"EllLdY8Y1gZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explore Numerical Values\n","df.hist(figsize=(20,25))\n","plt.title('Histograms of features')\n","plt.show()"],"metadata":{"id":"FMhxAv0x1tTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Handling Skewed Data\n","\n","The dataset exhibits some skewness, which can negatively impact model performance. To address this, we can apply various transformation techniques:  \n","\n","### **Techniques to Handle Skewness**\n","- **Log Transformation**: Useful for right-skewed distributions (e.g., `np.log1p(column)`).  \n","- **Square Root Transformation**: Reduces skewness while preserving relationships.\n","\n","Applying the appropriate transformation can help normalize the data and improve model performance.\n"],"metadata":{"id":"Yez_LaHUqQL0"}},{"cell_type":"code","source":["rows = len(numeric_cols) // 2 if len(numeric_cols) % 2 == 0 else (len(numeric_cols.columns) // 2) + 1\n","\n","plt.figure(figsize=(10, rows * 4))\n","\n","for i, col in enumerate(numeric_cols):\n","    plt.subplot(rows, 2, i + 1)\n","    sns.boxplot(y=df[col])\n","    plt.ylabel(col)\n","    plt.title(f\"Boxplot of column  {col}\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"kVxM1JmkqTQt"},"execution_count":null,"outputs":[]},{"source":["plt.figure(figsize=(20,10))\n","corr=df.corr()\n","sns.heatmap(corr,annot=True)\n","plt.title('Correlation Heatmap')\n","plt.show()"],"cell_type":"code","metadata":{"id":"E0eErrng4xB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corr.abs()"],"metadata":{"id":"tfCTO_df2Uyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key , value in df.items():\n","    qua1 = value.quantile(0.25)\n","    qua3 = value.quantile(0.75)\n","    iqr = qua3 - qua1\n","    value_col = value[(value <= qua1 - 1.5 * iqr) | (value >= qua3 + 1.5 * iqr)]\n","    percentage = np.shape(value_col)[0] * 100.0 / np.shape(df)[0]\n","    print(\"Column %s outliers = %.2f%%\" % (key, percentage))"],"metadata":{"id":"riGpoTNArcAQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Handling Outliers in the Boston Housing Dataset  \n","\n","During our analysis, we discovered that **a significant portion of the dataset (~283 rows out of 506)** were identified as outliers using the **Interquartile Range (IQR) method**.  \n","Since removing such a large portion of data could lead to **data loss and reduced model performance**, we opted for a more robust approach.  \n","\n","### **Our Approach to Handling Outliers:**  \n","1. **Log Transformation**:  \n","   - Many numerical features, such as `CRIM`, `LSTAT`, and `B`, exhibit **high skewness**.  \n","   - We apply a **log transformation** (`log(1 + x)`) to reduce the impact of extreme values.  \n","\n","2. **Robust Scaling**:  \n","   - Instead of using standard normalization methods (like MinMax or StandardScaler), we apply **RobustScaler**, which scales data using the **median and IQR**.  \n","   - This makes our dataset **more stable** for machine learning models by reducing the influence of outliers.  \n","\n","### **Why This Works?**  \n","\n","*   Preserves valuable data instead of removing outliers.  \n","\n","*  Makes the distribution more **normal-like**, improving model performance.\n","\n","*    Reduces the impact of extreme values, ensuring robust feature scaling.\n","\n","\n","\n","\n",""],"metadata":{"id":"eK_Kk6Vfrfbc"}},{"source":["# Define skewed columns\n","skewed_cols = [\"AGE\", \"DIS\", \"CRIM\", \"LSTAT\", \"B\"]\n","\n","# Apply log1p transformation for better stability\n","for col in skewed_cols:\n","    df[col] = np.log(df[col]+1e-10)\n","\n","# Plot histograms after log transformation\n","df.hist(figsize=(18, 10))\n","plt.suptitle(\"Histogram of Skewed Columns After Log Transformation\", fontsize=16)\n","plt.show()"],"cell_type":"code","metadata":{"id":"pb5jZpIi5Zcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_univariate= df[['RM' ,'Price']]\n","sns.pairplot(df_univariate)"],"metadata":{"id":"QoH-Mb816oRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_displot = df[['INDUS','NOX', 'RM' ,'PTRATIO','LSTAT','Price']]\n","sns.pairplot(df_displot)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-28T05:10:18.062926Z","iopub.execute_input":"2022-12-28T05:10:18.0634Z","iopub.status.idle":"2022-12-28T05:10:26.548679Z","shell.execute_reply.started":"2022-12-28T05:10:18.063352Z","shell.execute_reply":"2022-12-28T05:10:26.547489Z"},"trusted":true,"id":"q0Y4Yc3fyB4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numerical_cols = [col for col in df.columns if col not in ['CHAS', 'RAD','Price']]\n","categorical_cols = ['CHAS', 'RAD']"],"metadata":{"id":"vszbRftg2zww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We apply **multiple sequential transformations** (such as imputation, log transformation, and scaling) to numerical features. A `Pipeline` is ideal for this because it ensures that each step is executed **in order**, preventing data leakage and maintaining consistency.  \n","\n","For categorical features, we apply **different transformations to different categorical columns** (e.g., One-Hot Encoding for `CHAS` and Ordinal Encoding for `RAD`). To handle this efficiently, we use a `ColumnTransformer`, which allows us to apply **specific transformations to specific groups of columns** within a single step."],"metadata":{"id":"iROIoqD4szp2"}},{"cell_type":"code","source":["# Numerical pipeline\n","numerical_pipline = Pipeline([\n","    (\"imputer\",SimpleImputer(strategy=\"median\")),\n","    (\"scaler\",RobustScaler())\n","])\n","\n","# Categorical pipeline\n","categorical_pipeline = ColumnTransformer([\n","    (\"chas\",OneHotEncoder(handle_unknown=\"ignore\",drop='first'),['CHAS']),\n","    ('rad',OrdinalEncoder(),['RAD'])\n","])\n","\n","# Combine both pipelines\n","preprocessor = ColumnTransformer([\n","    (\"cat\",categorical_pipeline,categorical_cols),\n","    (\"num\",numerical_pipline,numerical_cols)\n","])"],"metadata":{"id":"wXQb-VIW3pvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"MPJj8qeHt1-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Spliting target variable and independent variables\n","X=df.drop(['Price'],axis=1)\n","y=df[\"Price\"]\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"6dwwWemY3tRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_names = X_train.columns\n","X_train = preprocessor.fit_transform(X_train)\n","X_test = preprocessor.transform(X_test)"],"metadata":{"id":"v-A8Zgx56gvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize results dictionary for regression\n","results = {\n","    \"Model\": [],\n","    \"MSE\": [],\n","    \"RMSE\": [],\n","    \"MAE\": [],\n","    \"R2 Score\": [],\n","    \"CV R2 Score\": []\n","}\n","\n","# Function to append regression results\n","def append_results_regression(model_name, mse, rmse, mae, r2, cv_r2):\n","    results[\"Model\"].append(model_name)\n","    results[\"MSE\"].append(mse)\n","    results[\"RMSE\"].append(rmse)\n","    results[\"MAE\"].append(mae)\n","    results[\"R2 Score\"].append(r2)\n","    results[\"CV R2 Score\"].append(cv_r2)"],"metadata":{"id":"-W5ZYalW6-0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_evaluation_regression(y_pred, y_test, model, X_train, y_train):\n","    # Compute evaluation metrics for regression\n","    mse = mean_squared_error(y_test, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    r2 = r2_score(y_test, y_pred)\n","\n","    # Check if model is an sklearn model (i.e., has get_params())\n","    if hasattr(model, \"get_params\"):\n","        cross_score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n","    else:\n","        cross_score = None  # Cross-validation is not applicable for neural networks\n","\n","    # Print evaluation metrics\n","    print(f\"MSE: {mse:.4f}\")\n","    print(f\"RMSE: {rmse:.4f}\")\n","    print(f\"MAE: {mae:.4f}\")\n","    print(f\"R2 Score: {r2:.4f}\")\n","\n","    # Only print if cross-score is available\n","    if cross_score is not None:\n","        print(f\"Cross-Validation R2 Score: {cross_score:.4f}\")\n","\n","    # Optionally, display predictions and actual values side-by-side\n","    display(pd.DataFrame(np.c_[y_pred, y_test], columns=[\"Prediction\", \"Actual\"]).iloc[:20])\n","\n","\n","    # Return the metrics\n","    return mse, rmse, mae, r2, cross_score"],"metadata":{"id":"K-rhT0MA7ENq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_regression_results(y_test, y_pred):\n","\n","    # Compute residuals\n","    residuals = y_test - y_pred\n","\n","    # Set up the figure with 3 subplots\n","    plt.figure(figsize=(10, 15))\n","\n","    # True vs. Predicted values plot\n","    plt.subplot(3, 1, 1)\n","    plt.scatter(y_test, y_pred, alpha=0.7, edgecolor=\"k\")\n","    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n","    plt.xlabel(\"True Values\")\n","    plt.ylabel(\"Predicted Values\")\n","    plt.title(\"True vs. Predicted Values\")\n","\n","    # Residuals vs. Predicted values plot\n","    plt.subplot(3, 1, 2)\n","    plt.scatter(y_pred, residuals, alpha=0.7, edgecolor=\"k\")\n","    plt.axhline(0, color=\"r\", linestyle=\"--\")\n","    plt.xlabel(\"Predicted Values\")\n","    plt.ylabel(\"Residuals\")\n","    plt.title(\"Residuals vs. Predicted Values\")\n","\n","    # Histogram (and KDE) of residuals\n","    plt.subplot(3, 1, 3)\n","    sns.histplot(residuals, kde=True, color=\"b\", bins=20)\n","    plt.xlabel(\"Residuals\")\n","    plt.title(\"Distribution of Residuals\")\n","\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"QakyCUzB7SHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models We Will Apply:\n","\n","1. **Linear Regression**  \n","   A simple and interpretable model that assumes a linear relationship between features and the target variable.\n","\n","2. **Random Forest Regressor**  \n","   An ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting.\n","\n","3. **XGBoost Regressor**  \n","   An optimized gradient boosting algorithm that is known for high performance and efficiency in structured data.\n","\n","4. **SVM Regressor**  \n","   Uses Support Vector Machines for regression, aiming to find the best-fit hyperplane within a certain margin of error.\n","\n","5. **Ensemble Learning**  \n","   A combination of multiple models to enhance predictive performance by leveraging the strengths of each approach.\n"],"metadata":{"id":"vtNerVSvvZA0"}},{"cell_type":"markdown","source":["## Linear Regression\n"],"metadata":{"id":"WRwMUpjlvpLY"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","lr=LinearRegression()\n","lr.fit(X_train,y_train)\n","y_pred_lr=lr.predict(X_test)\n","coeffcients =pd.DataFrame([X.columns,lr.coef_]).T\n","coeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\n","coeffcients"],"metadata":{"id":"eybKcjEJ7YA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model evaluation\n","lr_mse, lr_rmse, lr_mae, lr_r2, lr_cross_score=model_evaluation_regression(y_pred_lr, y_test, lr, X_train, y_train)\n"],"metadata":{"id":"90DAdTbh7dJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a list for all modle results\n","append_results_regression('Linear Regression', lr_mse, lr_rmse, lr_mae, lr_r2, lr_cross_score)\n"],"metadata":{"id":"V0zS8hkL7lLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the results\n","plot_regression_results(y_test, y_pred_lr)"],"metadata":{"id":"Q7UcSJOG7nkN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest Regressor"],"metadata":{"id":"ntUCgWntx1a3"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","rf = RandomForestRegressor()\n","rf.fit(X_train, y_train)\n","y_pred_rf = rf.predict(X_test)\n","\n","\n","importance  = rf.feature_importances_\n","importance_df=pd.DataFrame({'Column':feature_names,'Importance':importance})\n","importance_df=importance_df.sort_values(by='Importance',ascending =False)\n","importance_df"],"metadata":{"id":"Ga8TrYLp7rl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_mse, rf_rmse, rf_mae, rf_r2, rf_cross_score=model_evaluation_regression(y_pred_rf, y_test, rf, X_train, y_train)\n"],"metadata":{"id":"fBJtaeMK7vZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["append_results_regression('Random Forest Regressor', rf_mse, rf_rmse, rf_mae, rf_r2, rf_cross_score)\n"],"metadata":{"id":"N3C7h7Mr8C-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_regression_results(y_test, y_pred_rf)\n"],"metadata":{"id":"DJhONlAO8JJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGBRegressor"],"metadata":{"id":"XMNeib4py9Tq"}},{"cell_type":"code","source":["from xgboost import XGBRegressor\n","xgb = XGBRegressor()\n","xgb.fit(X_train, y_train)\n","y_pred_xgb = xgb.predict(X_test)\n","\n","importance  = xgb.feature_importances_\n","importance_df=pd.DataFrame({'Column':feature_names,'Importance':importance})\n","importance_df=importance_df.sort_values(by='Importance',ascending =False)\n","importance_df"],"metadata":{"id":"Ls9c_r6b8ObD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb_mse, xgb_rmse, xgb_mae, xgb_r2, xgb_cross_score=model_evaluation_regression(y_pred_xgb, y_test, xgb, X_train, y_train)\n"],"metadata":{"id":"f6b-Ant88RO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["append_results_regression('XGBRegressor', xgb_mse, xgb_rmse, xgb_mae, xgb_r2, xgb_cross_score)\n"],"metadata":{"id":"E_aM8N4v8WE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_regression_results(y_test, y_pred_xgb)"],"metadata":{"id":"pPBWAczI8aFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SVM Regressor"],"metadata":{"id":"bzKu1pCOzaXy"}},{"cell_type":"code","source":["from sklearn import svm\n","svm = svm.SVR()\n","svm.fit(X_train, y_train)\n","y_pred_svm = svm.predict(X_test)"],"metadata":{"id":"pynMtOJp8eGt"},"execution_count":null,"outputs":[]},{"source":["svm_mse, svm_rmse, svm_mae, svm_r2, svm_cross_score=model_evaluation_regression(y_pred_svm, y_test, svm, X_train, y_train)\n"],"cell_type":"code","metadata":{"id":"hdjonrg99bm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["append_results_regression('SVM Regressor', svm_mse, svm_rmse, svm_mae, svm_r2, svm_cross_score)\n"],"metadata":{"id":"2-kPzQra9k6q"},"execution_count":null,"outputs":[]},{"source":["plot_regression_results(y_test, y_pred_svm)"],"cell_type":"code","metadata":{"id":"DmyPtZoE_yDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(results).sort_values(by='R2 Score',ascending=False)"],"metadata":{"id":"q4jgnInU0f9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  !git config --global user.email \"noumanyousuf0485@gmail.com\"\n","  !git config --global user.name \"NoumanYousaf14\"\n","\n","\n","\n","# !git commit -a -m \"Boston house data set and price prediction model is done\"\n"],"metadata":{"id":"JiWKBura2EIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"whcb7-pF2qnf"},"execution_count":null,"outputs":[]}]}